{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465fec36-c9fb-4ea8-becf-c2d58b9a32a2",
   "metadata": {},
   "source": [
    "# PCAI Use Case Demo - Supervised Fine-tuning\n",
    "## What is Supervised Fine-tuning?\n",
    "Supervised Fine-Tuning (SFT) involves training a pre-trained large language model on a labeled dataset with input-output pairs. The model learns to map inputs to the correct outputs by minimizing prediction errors. This process tailors the LLM to perform better on specific tasks or domains.\n",
    "Through supervised fine-tuning (SFT), we can achieve the following objectives:\n",
    "- SFT enables precise control over the model‚Äôs output format and style, ensuring consistency across responses.\n",
    "- In specialized domains, SFT helps tailor the model to meet specific requirements and adhere to domain-relevant standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b6f91-d5bf-4664-a653-47cde143993b",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "**1. Set Up Jupyter Notebook Instance with GPU**</br>\n",
    "Fine-tuning Large Language Models requires significant computational resources.\n",
    "Please create a Jupyter Notebook instance in PCAI with the following specifications:\n",
    "\n",
    "- 1 GPU (e.g., NVIDIA Tesla T4 or higher)\n",
    "- Sufficient CPU and RAM(e.g., at least 4 vCPUs & minimum 16 GB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb3285-d1fb-4465-9686-67d0d890b2f9",
   "metadata": {},
   "source": [
    "**2. Install Required Libraries**</br>\n",
    "Before running the demo, please install the necessary libraries in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57ac2b-6657-47f3-8d7e-b559a02a9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.56.2 mlflow==2.20.2 boto3==1.35.40 datasets torch==2.8.0 torchvision==0.23.0 trl==0.23.0 peft==0.17.1 bitsandbytes==0.48.1 accelerate==1.10.1 aioli-sdk==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699d797-42c5-4788-8611-1bbc075bf37c",
   "metadata": {},
   "source": [
    "***Library Overview:***\n",
    "- **Transformers:** Hugging Face‚Äôs open-source library for state-of-the-art language models and NLP tools.\n",
    "- **TRL (Transformers Reinforcement Learning)**: Hugging Face extension for advanced training methods, including reinforcement learning and supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a27d9-8268-4262-a41a-297c42f1b996",
   "metadata": {},
   "source": [
    "### Step 1. Prepare Data\n",
    "The supervised fine-tuning process requires a task-specific dataset structured with input-output pairs. Each pair should consist of:\n",
    "\n",
    "- An input prompt\n",
    "- The expected model response\n",
    "- Any additional context or metadata\n",
    "\n",
    "Supported Data format(https://huggingface.co/docs/trl/dataset_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506325a1-026f-4311-a5c7-2874bf21e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c44915-d065-41b8-914b-cec1ed35189e",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56af7c1e-49b5-405e-a055-6fb91316f548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rhgt1996/camel_math_split\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fab0b0-c9ef-4540-8dba-0c2d6e0f49b5",
   "metadata": {},
   "source": [
    "#### Convert to SFT compatible data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2803ba99-73b2-4c0a-9b4e-b85d01252fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a math tutor. Solve problems step by step.', 'role': 'system'}, {'content': \"Assuming that the student has basic knowledge of group theory, here's a question:\\n\\nProve that the group G of order 4 defined by the following Cayley table is isomorphic to either the cyclic group C4 or the Klein four-group V4:\\n\\n|  ùëí  |  ùëé  |  ùëè  |  ùëê  |\\n|:---:|:---:|:---:|:---:|\\n|  ùëí  |  ùëé  |  ùëè  |  ùëê  |\\n|  ùëé  |  ùëí  |  ùëê  |  ùëè  |\\n|  ùëè  |  ùëê  |  ùëí  |  ùëé  |\\n|  ùëê  |  ùëè  |  ùëé  |  ùëí  |\\n\\nFind the isomorphism between G and either C4 or V4, and state which one it is.\", 'role': 'user'}, {'content': \"First, let's recall the definitions of the cyclic group C4 and the Klein four-group V4.\\n\\nC4 = {1, x, x^2, x^3} with the following Cayley table:\\n\\n|  1  |  x  | x^2 | x^3 |\\n|:---:|:---:|:---:|:---:|\\n|  1  |  x  | x^2 | x^3 |\\n|  x  | x^2 | x^3 |  1  |\\n| x^2 | x^3 |  1  |  x  |\\n| x^3 |  1  |  x  | x^2 |\\n\\nV4 = {1, a, b, c} with the following Cayley table:\\n\\n|  1  |  a  |  b  |  c  |\\n|:---:|:---:|:---:|:---:|\\n|  1  |  a  |  b  |  c  |\\n|  a  |  1  |  c  |  b  |\\n|  b  |  c  |  1  |  a  |\\n|  c  |  b  |  a  |  1  |\\n\\nNow, let's compare the given group G with C4 and V4. We can see that the Cayley table of G is identical to the Cayley table of V4. Therefore, G is isomorphic to V4.\\n\\nTo find the isomorphism between G and V4, we can define a function œÜ: G ‚Üí V4 such that:\\n\\nœÜ(ùëí) = 1\\nœÜ(ùëé) = a\\nœÜ(ùëè) = b\\nœÜ(ùëê) = c\\n\\nSince the Cayley tables of G and V4 are identical, this function preserves the group operation, and it is bijective. Thus, œÜ is an isomorphism between G and V4.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "def convert_to_message_function(example):\n",
    "    prompt = {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"You are a math tutor. Solve problems step by step.\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': example['message_1']\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': example['message_2']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return prompt\n",
    "\n",
    "organized_dataset = dataset.map(convert_to_message_function, remove_columns=dataset.column_names['train'])\n",
    "print(organized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4b33d-d160-4a6e-a651-95af4f3fbbd8",
   "metadata": {},
   "source": [
    "### Step 2. Investigate the llm\n",
    "To effectively fine-tune a model, it‚Äôs important to understand its structure and configuration. For instance, knowing how chat templates work and applying them correctly is essential to ensure consistent and reliable results, and to avoid unexpected behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b970ca-031c-443c-919c-bdf4d89f5dd6",
   "metadata": {},
   "source": [
    "#### Load the model and check Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a8d97a-1df2-478d-9ef2-b30ae3488b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b39bfa2-5659-4dcb-9f8c-3558e591a18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781029ad-660e-4ac6-882f-3ce4e1e7550d",
   "metadata": {},
   "source": [
    "#### Inference with loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3336698d-759e-4245-9e50-a4ffd20bb910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{'role': 'system','content': \"You are a math tutor. Solve problems step by step.\"},{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be327669-87b6-4a02-b1be-b8a5462c0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Before training ***\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To solve this problem, we need to calculate the total time Mark spent on the sideline.\n",
      "\n",
      "First, we know that Mark played for 20 minutes.\n",
      "\n",
      "Next, we know that he rested after playing for 35 minutes.\n",
      "\n",
      "Now, we add the time he played and the time he rested to find the total time he spent on the sideline:\n",
      "\n",
      "20 minutes (played) + 35 minutes (rested) = 55 minutes\n",
      "\n",
      "So, Mark spent 55 minutes on the sideline.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs,max_new_tokens=500)\n",
    "print(\"*** Before training ***\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac289c8-946a-4ea4-8c94-85c657623446",
   "metadata": {},
   "source": [
    "### Step 3. Fine-tuning with LoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375c2e1-f44b-4e37-ba90-43d1f59efbbe",
   "metadata": {},
   "source": [
    "**LoRA (Low-Rank Adaptation)**</br>\n",
    "LoRA is a technique for efficiently fine-tuning large language models by injecting small trainable adapters (low-rank matrices) into certain layers of a pre-trained model. Instead of updating all the model parameters, LoRA only updates these lightweight adapters, drastically reducing the number of trainable parameters and computational resources required. This makes fine-tuning much faster and more cost-effective.\n",
    "\n",
    "**PEFT (Parameter-Efficient Fine-Tuning)**</br>\n",
    "PEFT is a broader category of techniques, including LoRA, that aim to fine-tune large models by updating only a small subset of parameters. Methods under PEFT (such as LoRA, adapters, prompt tuning, and others) allow users to adapt large models to new tasks with minimal computational expense and storage, making them practical for real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830bde4-27cd-410a-8beb-1ac79524483c",
   "metadata": {},
   "source": [
    "In this step, we will use the HuggingFace PEFT library to fine-tune a large language model using the LoRA technique.\n",
    "The primary focus of this tutorial is to demonstrate the fine-tuning process for LLMs within PCAI.\n",
    "For more in-depth information and advanced usage, please refer to the https://huggingface.co/docs/peft/index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3775eabc-f4a8-4760-a2dc-aeb523d76f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "import os\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c5c2-5bcc-483e-b5bd-ad3cfbbc0f5a",
   "metadata": {},
   "source": [
    "#### Leverage MLflow\n",
    "In PCAI, MLflow is available as part of the AI Essentials suite. We will utilize MLflow for logging training metrics and storing model artifacts.\n",
    "\n",
    "Hugging Face supports integration with MLflow through a callback mechanism, which we will take advantage of in this demo.\n",
    "\n",
    "To enable secure communication with MLflow, we will periodically refresh our JWT token for authentication and customize the Hugging Face callback function as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f5193c-a584-4cf6-948c-4de8e748b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN : [qDJpBYpLDrtZqBV0tnFA]\n"
     ]
    }
   ],
   "source": [
    "def renew_token(step: str = None):\n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "        os.environ['MLFLOW_TRACKING_TOKEN']=AUTH_TOKEN\n",
    "        os.environ[\"AWS_ACCESS_KEY_ID\"] = AUTH_TOKEN\n",
    "        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "        if step is not None:\n",
    "            logger.info(f\"AUTH_TOKEN - {step} : [{AUTH_TOKEN[-20:]}]\")\n",
    "        else:\n",
    "            logger.info(f\"AUTH_TOKEN : [{AUTH_TOKEN[-20:]}]\")\n",
    "\n",
    "renew_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a25809-6847-4a55-9870-858d6b4c07c6",
   "metadata": {},
   "source": [
    "**NOTE**</br>\n",
    "The MLflow Python SDK relies on the boto3 library to log artifacts. Once initialized, boto3 stores authentication details in its DEFAULT_SESSION.\n",
    "If the JWT token is refreshed, this cached session can lead to authentication issues. Therefore, whenever the JWT token is updated, we need to reset boto3‚Äôs DEFAULT_SESSION to ensure proper authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c5092b-178d-4940-901b-e62b5dedcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "class CustomizedMLflowCallback(MLflowCallback):\n",
    "    def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
    "        # self.renew_token()\n",
    "        renew_token('on_log')\n",
    "        super().on_log(args, state, control, logs, model=None, **kwargs)\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        # self.renew_token()\n",
    "        renew_token('on_save')\n",
    "        import boto3 \n",
    "        if boto3.DEFAULT_SESSION is not None:\n",
    "            logger.info(f\"boto3 : {boto3.DEFAULT_SESSION.get_credentials().access_key[-20:]}, Env : {os.environ['AWS_ACCESS_KEY_ID'][-20:]}\")\n",
    "            if boto3.DEFAULT_SESSION.get_credentials().access_key != os.environ['AWS_ACCESS_KEY_ID']:\n",
    "                boto3.DEFAULT_SESSION = None\n",
    "                logger.info(\"Initialize Default Session of Boto3 to update Credential from Environment Variable!\")\n",
    "            \n",
    "        super().on_save(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dad5f-06ac-46b3-b424-5a9cdaffd894",
   "metadata": {},
   "source": [
    "#### Setup the parameters related to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73aa5aa3-1b1f-4092-8598-deaf2f512d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "rank_dimension = 4 # rank dimension for LoRA update matrices (smaller = more compression)\n",
    "lora_alpha = 8 # lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_dropout = 0.05 # lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecd3ad6-7ef1-40d2-b8db-f035ae677718",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "model_dir = 'math-' + model_name.split('/')[1]\n",
    "# Configure trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=100, # Short step for demo\n",
    "    save_total_limit=5,\n",
    "    # num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=[],\n",
    "    max_length=max_seq_length,  # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd90d4b-60a9-4570-9a9e-635d387bcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=organized_dataset[\"train\"],\n",
    "    eval_dataset=organized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    ")\n",
    "trainer.add_callback(CustomizedMLflowCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e713d7-02be-496c-8118-759bcba64a95",
   "metadata": {},
   "source": [
    "#### Check how data is processed by trainer api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a4fc9a-b42a-44eb-ba06-01d381b9aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "Consider the metric space (X, d), where X is a set of real numbers and d is the usual distance function. Let f:X->X be defined by f(x) = x^2. Determine if the sequence {1, 1/2, 1/4, 1/8, ... } converges to a fixed point of f.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's first find the limit of the sequence {1, 1/2, 1/4, 1/8, ...}. This is a geometric sequence with the first term a = 1 and the common ratio r = 1/2. The formula for the nth term of a geometric sequence is:\n",
      "\n",
      "a_n = a * r^(n-1)\n",
      "\n",
      "As n approaches infinity, the limit of the sequence is:\n",
      "\n",
      "lim (n -> ‚àû) a * r^(n-1) = lim (n -> ‚àû) 1 * (1/2)^(n-1)\n",
      "\n",
      "Since the common ratio r is between -1 and 1, the limit of the sequence is 0:\n",
      "\n",
      "lim (n -> ‚àû) 1 * (1/2)^(n-1) = 0\n",
      "\n",
      "Now, let's check if this limit is a fixed point of the function f(x) = x^2. A fixed point is a point x* such that f(x*) = x*. In this case, we want to check if f(0) = 0:\n",
      "\n",
      "f(0) = 0^2 = 0\n",
      "\n",
      "Since f(0) = 0, the limit of the sequence {1, 1/2, 1/4, 1/8, ...} converges to a fixed point of the function f(x) = x^2.<|im_end|>\n",
      "<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(batch.keys())\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(batch['input_ids'][0],skip_special_tokens=False))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d139e4-fb57-4155-8768-dca57b99b94c",
   "metadata": {},
   "source": [
    "#### Launch the Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3167b9ef-10f5-41e1-82f3-5e144025a174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 47:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.596616</td>\n",
       "      <td>0.593281</td>\n",
       "      <td>205281.000000</td>\n",
       "      <td>0.826869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [QNWP1rkjDqtkPtsBURwA]\n",
      "AUTH_TOKEN - on_log : [wJwvYNEBSyAPOFxgJA1g]\n",
      "AUTH_TOKEN - on_log : [Awu0ZvUG4cZma4_2vNXA]\n",
      "AUTH_TOKEN - on_save : [Awu0ZvUG4cZma4_2vNXA]\n",
      "AUTH_TOKEN - on_log : [Awu0ZvUG4cZma4_2vNXA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run sedate-bear-555 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0/runs/e2358bb367144f5bb58026a5f61f145d\n",
      "üß™ View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.6048033046722412, metrics={'train_runtime': 2852.9636, 'train_samples_per_second': 0.14, 'train_steps_per_second': 0.035, 'total_flos': 614190680071680.0, 'train_loss': 0.6048033046722412, 'epoch': 0.01})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69e0ac-eb1a-431c-8bf6-295c53f98a75",
   "metadata": {},
   "source": [
    "<img src=\"../assets/mlflow_metrics.png\" alt=\"metrics in mlflow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb78518-2159-487b-8de6-7a3a96340140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(model_dir + 'savedir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b49c5268-f0a0-4f98-93ad-7ff9559ca4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN - Final Model Logging : [6wOPeTpfh_tX8V7kCO4A]\n",
      "Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run exultant-yak-716 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0/runs/1c8220945ef3485f83ed967df5d29963\n",
      "üß™ View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "## Log artifacts to MLFLOW\n",
    "import mlflow\n",
    "\n",
    "## Get the ID of the MLflow Run that was automatically created above\n",
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "renew_token('Final Model Logging')\n",
    "    \n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(peft_config.to_dict())\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "        artifact_path=model_dir + '-savedir',  # This is a relative path to save model files within MLflow run\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a6a0f-3063-477b-9d0c-64c1fe254134",
   "metadata": {},
   "source": [
    "<img src=\"../assets/mlflow_artifacts_1.png\" alt=\"metrics in mlflow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94270402-07ea-46c0-ad0c-637d77aba6c1",
   "metadata": {},
   "source": [
    "#### Inference with LoRa Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4009cee-086c-49b8-a74c-4f417ad17213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "instruct_finetuned_360 = AutoPeftModelForCausalLM.from_pretrained('./math-SmolLM2-360M-Instruct', device_map=\"auto\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e569006-3e19-4c18-b24a-5aec82156e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** With LoRa Adapter ***\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Mark played for 20 minutes + 35 minutes = 55 minutes on the sideline.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "outputs = instruct_finetuned_360.generate(**inputs,max_new_tokens=500)\n",
    "print(\"*** With LoRa Adapter ***\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
