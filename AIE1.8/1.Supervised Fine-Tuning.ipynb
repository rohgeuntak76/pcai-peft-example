{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465fec36-c9fb-4ea8-becf-c2d58b9a32a2",
   "metadata": {},
   "source": [
    "# PCAI Use Case Demo - Supervised Fine-tuning\n",
    "## What is Supervised Fine-tuning?\n",
    "Supervised Fine-Tuning (SFT) involves training a pre-trained large language model on a labeled dataset with input-output pairs. The model learns to map inputs to the correct outputs by minimizing prediction errors. This process tailors the LLM to perform better on specific tasks or domains.\n",
    "Through supervised fine-tuning (SFT), we can achieve the following objectives:\n",
    "- SFT enables precise control over the modelâ€™s output format and style, ensuring consistency across responses.\n",
    "- In specialized domains, SFT helps tailor the model to meet specific requirements and adhere to domain-relevant standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b6f91-d5bf-4664-a653-47cde143993b",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "**1. Set Up Jupyter Notebook Instance with GPU**</br>\n",
    "Fine-tuning Large Language Models requires significant computational resources.\n",
    "Please create a Jupyter Notebook instance in PCAI with the following specifications:\n",
    "\n",
    "- 1 GPU (e.g., NVIDIA Tesla T4 or higher)\n",
    "- Sufficient CPU and RAM(e.g., at least 4 vCPUs & minimum 16 GB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb3285-d1fb-4465-9686-67d0d890b2f9",
   "metadata": {},
   "source": [
    "**2. Install Required Libraries**</br>\n",
    "Before running the demo, please install the necessary libraries in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a57ac2b-6657-47f3-8d7e-b559a02a9275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.56.2\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: mlflow==2.20.2 in /opt/conda/lib/python3.11/site-packages (2.20.2)\n",
      "Requirement already satisfied: boto3==1.35.40 in /opt/conda/lib/python3.11/site-packages (1.35.40)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch==2.8.0\n",
      "  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision==0.23.0\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting trl==0.23.0\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft==0.17.1\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes==0.48.1\n",
      "  Using cached bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting accelerate==1.10.1\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aioli-sdk==1.10.0\n",
      "  Downloading aioli_sdk-1.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock (from transformers==4.56.2)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.56.2)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.56.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers==4.56.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.56.2) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.56.2)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.56.2) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.56.2)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.56.2)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers==4.56.2) (4.67.1)\n",
      "Requirement already satisfied: mlflow-skinny==2.20.2 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (2.20.2)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (3.1.6)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (1.15.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (3.4.3)\n",
      "Requirement already satisfied: gunicorn<24 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (23.0.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (3.8.4)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (2.1.4)\n",
      "Requirement already satisfied: pyarrow<19,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (11.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (1.3.2)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow==2.20.2) (2.0.30)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.40 in /opt/conda/lib/python3.11/site-packages (from boto3==1.35.40) (1.35.99)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3==1.35.40) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3==1.35.40) (0.10.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0) (4.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0)\n",
      "  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision==0.23.0) (11.1.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=18.1.0 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (26.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (2025.1.31)\n",
      "Collecting lomond>=0.3.3 (from aioli-sdk==1.10.0)\n",
      "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pathspec>=0.6.0 (from aioli-sdk==1.10.0)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting termcolor>=1.1.0 (from aioli-sdk==1.10.0)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting argcomplete>=1.9.4 (from aioli-sdk==1.10.0)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: gitpython>=3.1.3 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (3.1.44)\n",
      "Collecting pyOpenSSL>=19.1.0 (from aioli-sdk==1.10.0)\n",
      "  Downloading pyopenssl-25.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (2025.2)\n",
      "Requirement already satisfied: tabulate>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (0.9.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.15.29 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (0.18.10)\n",
      "Collecting paramiko>=2.4.2 (from aioli-sdk==1.10.0)\n",
      "  Downloading paramiko-4.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting appdirs (from aioli-sdk==1.10.0)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting analytics-python (from aioli-sdk==1.10.0)\n",
      "  Downloading analytics_python-1.4.post1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting urllib3<2.3.0,>=2.0.0 (from aioli-sdk==1.10.0)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (2.11.3)\n",
      "Collecting pytest>=7.4.4 (from aioli-sdk==1.10.0)\n",
      "  Downloading pytest-9.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pytest-cov>=4.1.0 (from aioli-sdk==1.10.0)\n",
      "  Downloading pytest_cov-7.0.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: pexpect>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from aioli-sdk==1.10.0) (4.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (2.2.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (0.49.0)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (8.6.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (1.32.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (4.25.6)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.20.2->mlflow==2.20.2) (0.5.3)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.4.0->torch==2.8.0) (78.1.0)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyarrow<19,>=4.0.0 (from mlflow==2.20.2)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.8.0)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow==2.20.2) (1.3.10)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow==2.20.2) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow==2.20.2) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow==2.20.2) (1.9.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython>=3.1.3->aioli-sdk==1.10.0) (4.0.12)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow==2.20.2) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow==2.20.2) (3.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow==2.20.2) (3.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from lomond>=0.3.3->aioli-sdk==1.10.0) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.20.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.20.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.20.2) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.20.2) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow==2.20.2) (3.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3->mlflow==2.20.2) (2025.2)\n",
      "Collecting bcrypt>=3.2 (from paramiko>=2.4.2->aioli-sdk==1.10.0)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.11/site-packages (from paramiko>=2.4.2->aioli-sdk==1.10.0) (44.0.2)\n",
      "Collecting invoke>=2.0 (from paramiko>=2.4.2->aioli-sdk==1.10.0)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko>=2.4.2->aioli-sdk==1.10.0)\n",
      "  Downloading pynacl-1.6.1-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>=4.9.0->aioli-sdk==1.10.0) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->aioli-sdk==1.10.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->aioli-sdk==1.10.0) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2->aioli-sdk==1.10.0) (0.4.0)\n",
      "Collecting cryptography>=3.3 (from paramiko>=2.4.2->aioli-sdk==1.10.0)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting iniconfig>=1.0.1 (from pytest>=7.4.4->aioli-sdk==1.10.0)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.11/site-packages (from pytest>=7.4.4->aioli-sdk==1.10.0) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from pytest>=7.4.4->aioli-sdk==1.10.0) (2.19.1)\n",
      "Collecting coverage>=7.10.6 (from coverage[toml]>=7.10.6->pytest-cov>=4.1.0->aioli-sdk==1.10.0)\n",
      "  Downloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.56.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.56.2) (3.10)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from ruamel.yaml>=0.15.29->aioli-sdk==1.10.0) (0.2.8)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.20.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow==2.20.2) (3.6.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.20.2) (3.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.11/site-packages (from analytics-python->aioli-sdk==1.10.0) (1.6)\n",
      "Collecting backoff==1.10.0 (from analytics-python->aioli-sdk==1.10.0)\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=3.3->paramiko>=2.4.2->aioli-sdk==1.10.0)\n",
      "  Downloading cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (2.38.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.3->aioli-sdk==1.10.0) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (0.53b0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=2.0.0->cryptography>=3.3->paramiko>=2.4.2->aioli-sdk==1.10.0) (2.22)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (1.17.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.2->mlflow==2.20.2) (0.6.1)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading aioli_sdk-1.10.0-py3-none-any.whl (158 kB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading paramiko-4.0.0-py3-none-any.whl (223 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
      "Downloading pytest-9.0.1-py3-none-any.whl (373 kB)\n",
      "Downloading pytest_cov-7.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading analytics_python-1.4.post1-py2.py3-none-any.whl (23 kB)\n",
      "Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "Downloading coverage-7.11.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (249 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading pynacl-1.6.1-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (215 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, appdirs, xxhash, urllib3, triton, termcolor, safetensors, regex, pyarrow, propcache, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, lomond, invoke, iniconfig, hf-xet, fsspec, frozenlist, filelock, dill, coverage, cffi, bcrypt, backoff, argcomplete, aiohappyeyeballs, yarl, pytest, pynacl, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, cryptography, aiosignal, pytest-cov, pyOpenSSL, paramiko, nvidia-cusolver-cu12, huggingface-hub, analytics-python, aiohttp, torch, tokenizers, transformers, torchvision, datasets, bitsandbytes, aioli-sdk, accelerate, trl, peft\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.20\n",
      "    Uninstalling urllib3-1.26.20:\n",
      "      Successfully uninstalled urllib3-1.26.20\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.17.1\n",
      "    Uninstalling cffi-1.17.1:\n",
      "      Successfully uninstalled cffi-1.17.1\n",
      "  Attempting uninstall: backoff\n",
      "    Found existing installation: backoff 2.2.1\n",
      "    Uninstalling backoff-2.2.1:\n",
      "      Successfully uninstalled backoff-2.2.1\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 44.0.2\n",
      "    Uninstalling cryptography-44.0.2:\n",
      "      Successfully uninstalled cryptography-44.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.9.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aioli-sdk-1.10.0 aiosignal-1.4.0 analytics-python-1.4.post1 appdirs-1.4.4 argcomplete-3.6.3 backoff-1.10.0 bcrypt-5.0.0 bitsandbytes-0.48.1 cffi-2.0.0 coverage-7.11.3 cryptography-46.0.3 datasets-4.0.0 dill-0.3.8 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.3.0 hf-xet-1.2.0 huggingface-hub-0.36.0 iniconfig-2.3.0 invoke-2.2.1 lomond-0.3.3 multidict-6.7.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 paramiko-4.0.0 pathspec-0.12.1 peft-0.17.1 propcache-0.4.1 pyOpenSSL-25.3.0 pyarrow-18.1.0 pynacl-1.6.1 pytest-9.0.1 pytest-cov-7.0.0 regex-2025.11.3 safetensors-0.6.2 termcolor-3.2.0 tokenizers-0.22.1 torch-2.8.0 torchvision-0.23.0 transformers-4.56.2 triton-3.4.0 trl-0.23.0 urllib3-2.2.3 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.56.2 mlflow==2.20.2 boto3==1.35.40 datasets torch==2.8.0 torchvision==0.23.0 trl==0.23.0 peft==0.17.1 bitsandbytes==0.48.1 accelerate==1.10.1 aioli-sdk==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699d797-42c5-4788-8611-1bbc075bf37c",
   "metadata": {},
   "source": [
    "***Library Overview:***\n",
    "- **Transformers:** Hugging Faceâ€™s open-source library for state-of-the-art language models and NLP tools.\n",
    "- **TRL (Transformers Reinforcement Learning)**: Hugging Face extension for advanced training methods, including reinforcement learning and supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a27d9-8268-4262-a41a-297c42f1b996",
   "metadata": {},
   "source": [
    "### Step 1. Prepare Data\n",
    "The supervised fine-tuning process requires a task-specific dataset structured with input-output pairs. Each pair should consist of:\n",
    "\n",
    "- An input prompt\n",
    "- The expected model response\n",
    "- Any additional context or metadata\n",
    "\n",
    "Supported Data format(https://huggingface.co/docs/trl/dataset_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506325a1-026f-4311-a5c7-2874bf21e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c44915-d065-41b8-914b-cec1ed35189e",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56af7c1e-49b5-405e-a055-6fb91316f548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb14ec5fc174ea7bb239da3c825c469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82681eae70d4ce190a521ce06953b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/28.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe3c4f2e613426da5ff9a3998205e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/3.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c17058878284529acf023d032806ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b513f2b6e64341a5eff4350b0ace98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07e022026114bf6be680ba5848cf6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea075b50be64986a9eaeb4bbaedbf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['role_1', 'topic;', 'sub_topic', 'message_1', 'message_2'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rhgt1996/camel_math_split\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fab0b0-c9ef-4540-8dba-0c2d6e0f49b5",
   "metadata": {},
   "source": [
    "#### Convert to SFT compatible data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2803ba99-73b2-4c0a-9b4e-b85d01252fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1758efb96f437bb43e41a71fbd4b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137a87b859f94ffb839f364ab3f8c880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51274b701ce54daca32e5787e86a14ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a math tutor. Solve problems step by step.', 'role': 'system'}, {'content': \"Assuming that the student has basic knowledge of group theory, here's a question:\\n\\nProve that the group G of order 4 defined by the following Cayley table is isomorphic to either the cyclic group C4 or the Klein four-group V4:\\n\\n|  ğ‘’  |  ğ‘  |  ğ‘  |  ğ‘  |\\n|:---:|:---:|:---:|:---:|\\n|  ğ‘’  |  ğ‘  |  ğ‘  |  ğ‘  |\\n|  ğ‘  |  ğ‘’  |  ğ‘  |  ğ‘  |\\n|  ğ‘  |  ğ‘  |  ğ‘’  |  ğ‘  |\\n|  ğ‘  |  ğ‘  |  ğ‘  |  ğ‘’  |\\n\\nFind the isomorphism between G and either C4 or V4, and state which one it is.\", 'role': 'user'}, {'content': \"First, let's recall the definitions of the cyclic group C4 and the Klein four-group V4.\\n\\nC4 = {1, x, x^2, x^3} with the following Cayley table:\\n\\n|  1  |  x  | x^2 | x^3 |\\n|:---:|:---:|:---:|:---:|\\n|  1  |  x  | x^2 | x^3 |\\n|  x  | x^2 | x^3 |  1  |\\n| x^2 | x^3 |  1  |  x  |\\n| x^3 |  1  |  x  | x^2 |\\n\\nV4 = {1, a, b, c} with the following Cayley table:\\n\\n|  1  |  a  |  b  |  c  |\\n|:---:|:---:|:---:|:---:|\\n|  1  |  a  |  b  |  c  |\\n|  a  |  1  |  c  |  b  |\\n|  b  |  c  |  1  |  a  |\\n|  c  |  b  |  a  |  1  |\\n\\nNow, let's compare the given group G with C4 and V4. We can see that the Cayley table of G is identical to the Cayley table of V4. Therefore, G is isomorphic to V4.\\n\\nTo find the isomorphism between G and V4, we can define a function Ï†: G â†’ V4 such that:\\n\\nÏ†(ğ‘’) = 1\\nÏ†(ğ‘) = a\\nÏ†(ğ‘) = b\\nÏ†(ğ‘) = c\\n\\nSince the Cayley tables of G and V4 are identical, this function preserves the group operation, and it is bijective. Thus, Ï† is an isomorphism between G and V4.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "def convert_to_message_function(example):\n",
    "    prompt = {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"You are a math tutor. Solve problems step by step.\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': example['message_1']\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': example['message_2']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return prompt\n",
    "\n",
    "organized_dataset = dataset.map(convert_to_message_function, remove_columns=dataset.column_names['train'])\n",
    "print(organized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4b33d-d160-4a6e-a651-95af4f3fbbd8",
   "metadata": {},
   "source": [
    "### Step 2. Investigate the llm\n",
    "To effectively fine-tune a model, itâ€™s important to understand its structure and configuration. For instance, knowing how chat templates work and applying them correctly is essential to ensure consistent and reliable results, and to avoid unexpected behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b970ca-031c-443c-919c-bdf4d89f5dd6",
   "metadata": {},
   "source": [
    "#### Load the model and check Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a8d97a-1df2-478d-9ef2-b30ae3488b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce2f7c7403041f4a0192115b6436478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d286e5524a1c4f509497f74f1b28fb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467a7593725748d887568e13de5fe3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c631fe9307456dbc8b1194d83988c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8c151a5e8414aa732cb987168d56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba58b0a6ee3048a5b97de3e161f866b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c20456a215419ab8608b334733e12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f566f045fa441cbee3c1991b4909d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b39bfa2-5659-4dcb-9f8c-3558e591a18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781029ad-660e-4ac6-882f-3ce4e1e7550d",
   "metadata": {},
   "source": [
    "#### Inference with loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3336698d-759e-4245-9e50-a4ffd20bb910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{'role': 'system','content': \"You are a math tutor. Solve problems step by step.\"},{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be327669-87b6-4a02-b1be-b8a5462c0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Before training ***\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To solve this problem, we need to calculate the total time Mark spent on the sideline.\n",
      "\n",
      "First, we know that Mark played for 20 minutes.\n",
      "\n",
      "Next, we know that he rested after playing for 35 minutes.\n",
      "\n",
      "Now, we add the time he played and the time he rested to find the total time he spent on the sideline:\n",
      "\n",
      "20 minutes (played) + 35 minutes (rested) = 55 minutes\n",
      "\n",
      "So, Mark spent 55 minutes on the sideline.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs,max_new_tokens=500)\n",
    "print(\"*** Before training ***\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac289c8-946a-4ea4-8c94-85c657623446",
   "metadata": {},
   "source": [
    "### Step 3. Fine-tuning with LoRa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375c2e1-f44b-4e37-ba90-43d1f59efbbe",
   "metadata": {},
   "source": [
    "**LoRA (Low-Rank Adaptation)**</br>\n",
    "LoRA is a technique for efficiently fine-tuning large language models by injecting small trainable adapters (low-rank matrices) into certain layers of a pre-trained model. Instead of updating all the model parameters, LoRA only updates these lightweight adapters, drastically reducing the number of trainable parameters and computational resources required. This makes fine-tuning much faster and more cost-effective.\n",
    "\n",
    "**PEFT (Parameter-Efficient Fine-Tuning)**</br>\n",
    "PEFT is a broader category of techniques, including LoRA, that aim to fine-tune large models by updating only a small subset of parameters. Methods under PEFT (such as LoRA, adapters, prompt tuning, and others) allow users to adapt large models to new tasks with minimal computational expense and storage, making them practical for real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830bde4-27cd-410a-8beb-1ac79524483c",
   "metadata": {},
   "source": [
    "In this step, we will use the HuggingFace PEFT library to fine-tune a large language model using the LoRA technique.\n",
    "The primary focus of this tutorial is to demonstrate the fine-tuning process for LLMs within PCAI.\n",
    "For more in-depth information and advanced usage, please refer to the https://huggingface.co/docs/peft/index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3775eabc-f4a8-4760-a2dc-aeb523d76f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "import os\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c5c2-5bcc-483e-b5bd-ad3cfbbc0f5a",
   "metadata": {},
   "source": [
    "#### Leverage MLflow\n",
    "In PCAI, MLflow is available as part of the AI Essentials suite. We will utilize MLflow for logging training metrics and storing model artifacts.\n",
    "\n",
    "Hugging Face supports integration with MLflow through a callback mechanism, which we will take advantage of in this demo.\n",
    "\n",
    "To enable secure communication with MLflow, we will periodically refresh our JWT token for authentication and customize the Hugging Face callback function as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f5193c-a584-4cf6-948c-4de8e748b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN : [8OXSwwGZJvXJuLWJItEA]\n"
     ]
    }
   ],
   "source": [
    "def renew_token(step: str = None):\n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "        os.environ['MLFLOW_TRACKING_TOKEN']=AUTH_TOKEN\n",
    "        os.environ[\"AWS_ACCESS_KEY_ID\"] = AUTH_TOKEN\n",
    "        os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "        if step is not None:\n",
    "            logger.info(f\"AUTH_TOKEN - {step} : [{AUTH_TOKEN[-20:]}]\")\n",
    "        else:\n",
    "            logger.info(f\"AUTH_TOKEN : [{AUTH_TOKEN[-20:]}]\")\n",
    "\n",
    "renew_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a25809-6847-4a55-9870-858d6b4c07c6",
   "metadata": {},
   "source": [
    "**NOTE**</br>\n",
    "The MLflow Python SDK relies on the boto3 library to log artifacts. Once initialized, boto3 stores authentication details in its DEFAULT_SESSION.\n",
    "If the JWT token is refreshed, this cached session can lead to authentication issues. Therefore, whenever the JWT token is updated, we need to reset boto3â€™s DEFAULT_SESSION to ensure proper authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c5092b-178d-4940-901b-e62b5dedcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "class CustomizedMLflowCallback(MLflowCallback):\n",
    "    def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
    "        # self.renew_token()\n",
    "        renew_token('on_log')\n",
    "        super().on_log(args, state, control, logs, model=None, **kwargs)\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        # self.renew_token()\n",
    "        renew_token('on_save')\n",
    "        import boto3 \n",
    "        if boto3.DEFAULT_SESSION is not None:\n",
    "            logger.info(f\"boto3 : {boto3.DEFAULT_SESSION.get_credentials().access_key[-20:]}, Env : {os.environ['AWS_ACCESS_KEY_ID'][-20:]}\")\n",
    "            if boto3.DEFAULT_SESSION.get_credentials().access_key != os.environ['AWS_ACCESS_KEY_ID']:\n",
    "                boto3.DEFAULT_SESSION = None\n",
    "                logger.info(\"Initialize Default Session of Boto3 to update Credential from Environment Variable!\")\n",
    "            \n",
    "        super().on_save(args, state, control, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dad5f-06ac-46b3-b424-5a9cdaffd894",
   "metadata": {},
   "source": [
    "#### Setup the parameters related to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73aa5aa3-1b1f-4092-8598-deaf2f512d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# TODO: Configure LoRA parameters\n",
    "rank_dimension = 4 # rank dimension for LoRA update matrices (smaller = more compression)\n",
    "lora_alpha = 8 # lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_dropout = 0.05 # lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecd3ad6-7ef1-40d2-b8db-f035ae677718",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "model_dir = 'math-' + model_name.split('/')[1]\n",
    "# Configure trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=100, # Short step for demo\n",
    "    save_total_limit=5,\n",
    "    # num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=[],\n",
    "    max_length=max_seq_length,  # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd90d4b-60a9-4570-9a9e-635d387bcbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2e299c2f4041a780db2e7337c4b394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede4b738eefc4961889760692d695b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12e7c6b232348ccbf1cf5aa5880b753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eab0baf0b94bab8f91505e7a3178f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=organized_dataset[\"train\"],\n",
    "    eval_dataset=organized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    ")\n",
    "trainer.add_callback(CustomizedMLflowCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e713d7-02be-496c-8118-759bcba64a95",
   "metadata": {},
   "source": [
    "#### Check how data is processed by trainer api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a4fc9a-b42a-44eb-ba06-01d381b9aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "\n",
      "\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "Consider the metric space (X, d), where X is a set of real numbers and d is the usual distance function. Let f:X->X be defined by f(x) = x^2. Determine if the sequence {1, 1/2, 1/4, 1/8, ... } converges to a fixed point of f.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's first find the limit of the sequence {1, 1/2, 1/4, 1/8, ...}. This is a geometric sequence with the first term a = 1 and the common ratio r = 1/2. The formula for the nth term of a geometric sequence is:\n",
      "\n",
      "a_n = a * r^(n-1)\n",
      "\n",
      "As n approaches infinity, the limit of the sequence is:\n",
      "\n",
      "lim (n -> âˆ) a * r^(n-1) = lim (n -> âˆ) 1 * (1/2)^(n-1)\n",
      "\n",
      "Since the common ratio r is between -1 and 1, the limit of the sequence is 0:\n",
      "\n",
      "lim (n -> âˆ) 1 * (1/2)^(n-1) = 0\n",
      "\n",
      "Now, let's check if this limit is a fixed point of the function f(x) = x^2. A fixed point is a point x* such that f(x*) = x*. In this case, we want to check if f(0) = 0:\n",
      "\n",
      "f(0) = 0^2 = 0\n",
      "\n",
      "Since f(0) = 0, the limit of the sequence {1, 1/2, 1/4, 1/8, ...} converges to a fixed point of the function f(x) = x^2.<|im_end|>\n",
      "<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(batch.keys())\n",
    "    print(\"\\n\")\n",
    "    print(tokenizer.decode(batch['input_ids'][0],skip_special_tokens=False))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d139e4-fb57-4155-8768-dca57b99b94c",
   "metadata": {},
   "source": [
    "#### Launch the Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3167b9ef-10f5-41e1-82f3-5e144025a174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.596538</td>\n",
       "      <td>0.592354</td>\n",
       "      <td>205281.000000</td>\n",
       "      <td>0.826857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_save : [8OXSwwGZJvXJuLWJItEA]\n",
      "AUTH_TOKEN - on_log : [8OXSwwGZJvXJuLWJItEA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run blushing-horse-200 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0/runs/d7a0b54b7f334a1bac0ce99dfec37e31\n",
      "ğŸ§ª View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.6047144556045532, metrics={'train_runtime': 103.3002, 'train_samples_per_second': 3.872, 'train_steps_per_second': 0.968, 'total_flos': 614190680071680.0, 'train_loss': 0.6047144556045532, 'epoch': 0.01})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69e0ac-eb1a-431c-8bf6-295c53f98a75",
   "metadata": {},
   "source": [
    "<img src=\"../assets/mlflow_metrics.png\" alt=\"metrics in mlflow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb78518-2159-487b-8de6-7a3a96340140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b49c5268-f0a0-4f98-93ad-7ff9559ca4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTH_TOKEN - Final Model Logging : [6wOPeTpfh_tX8V7kCO4A]\n",
      "Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run exultant-yak-716 at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0/runs/1c8220945ef3485f83ed967df5d29963\n",
      "ğŸ§ª View experiment at: http://mlflow.mlflow.svc.cluster.local:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "## Log artifacts to MLFLOW\n",
    "import mlflow\n",
    "\n",
    "## Get the ID of the MLflow Run that was automatically created above\n",
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "renew_token('Final Model Logging')\n",
    "    \n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(peft_config.to_dict())\n",
    "    mlflow.log_artifacts(model_dir, artifact_path=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a6a0f-3063-477b-9d0c-64c1fe254134",
   "metadata": {},
   "source": [
    "<img src=\"../assets/mlflow_artifacts.png\" alt=\"metrics in mlflow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94270402-07ea-46c0-ad0c-637d77aba6c1",
   "metadata": {},
   "source": [
    "#### Inference with LoRa Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4009cee-086c-49b8-a74c-4f417ad17213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "instruct_finetuned_360 = AutoPeftModelForCausalLM.from_pretrained('./math-SmolLM2-360M-Instruct', device_map=\"auto\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e569006-3e19-4c18-b24a-5aec82156e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** With LoRa Adapter ***\n",
      "<|im_start|>system\n",
      "You are a math tutor. Solve problems step by step.<|im_end|>\n",
      "<|im_start|>user\n",
      "In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Mark played for 20 minutes + 35 minutes = 55 minutes on the sideline.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "outputs = instruct_finetuned_360.generate(**inputs,max_new_tokens=500)\n",
    "print(\"*** With LoRa Adapter ***\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
