{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1cff6f-6174-473d-a91d-a32d8a3d90bc",
   "metadata": {},
   "source": [
    "# PCAI Use Case Demo - End to End demo with Kubeflow Pipeline\n",
    "In this tutorial, we will implement previous steps into Kubeflow Pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b48a93-84db-4899-8dec-5661cfaa7ba6",
   "metadata": {},
   "source": [
    "## What is Kubeflow Pipeline?\n",
    "Kubeflow Pipelines (KFP) is a platform for building and deploying portable and scalable machine learning (ML) workflows using containers on Kubernetes-based systems.\n",
    "\n",
    "With KFP you can author components and pipelines using the KFP Python SDK, compile pipelines to an intermediate representation YAML, and submit the pipeline to run on a KFP-conformant backend.\n",
    "\n",
    "## Why Kubeflow Pipelines?\n",
    "KFP enables data scientists and machine learning engineers to:\n",
    "- Author end-to-end ML workflows natively in Python\n",
    "- Create fully custom ML components or leverage an ecosystem of existing components\n",
    "- Easily manage, track, and visualize pipeline definitions, runs, experiments, and ML artifacts\n",
    "\n",
    "Ref: https://www.kubeflow.org/docs/components/pipelines/overview/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc7cb3-c831-42db-a92a-28a0f2d4cbdc",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "**1. Install Required Libraries**</br>\n",
    "Before running the demo, please install the necessary libraries in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10e1a1f-ac08-46bc-a342-7597602a84be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp-kubernetes==1.4.0 in /opt/conda/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from kfp-kubernetes==1.4.0) (4.25.6)\n",
      "Requirement already satisfied: kfp<3,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from kfp-kubernetes==1.4.0) (2.9.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (8.1.8)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.38.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.19.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.4.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.4.0)\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.3.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (27.2.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.26.20)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.69.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.4.0,>=2.1.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp<3,>=2.6.0->kfp-kubernetes==1.4.0) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp-kubernetes==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521793d-011c-4d6f-bb28-a344f1d66b27",
   "metadata": {},
   "source": [
    "# 1. Define each steps as a KFP Components\n",
    "A pipeline component is self-contained set of code that performs one step in the ML workflow (pipeline), such as data preprocessing, data transformation, model training, and so on. A component is analogous to a function, in that it has a name, parameters, return values, and a body.\n",
    "The code for each component includes the following:\n",
    "- Client code: The code that talks to endpoints to submit jobs. For example, code to talk to the Google Dataproc API to submit a Spark job.\n",
    "- Runtime code: The code that does the actual job and usually runs in the cluster. For example, Spark code that transforms raw data into preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cb729b-e40a-4491-abe2-143a4edee5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token successfully refreshed.\n"
     ]
    }
   ],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69951f5-f559-41e5-a24d-fc128ed0fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a010ef-1413-48c2-b5ce-ee6060cc510c",
   "metadata": {},
   "source": [
    "### Component #1 Preparing Data for SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41604b78-0bbe-4c60-a623-ce1b841abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='geuntakroh/transformers-pytorch-gpu:v4.57.1',\n",
    ")\n",
    "def preprocess_dataset(dataset_name: str, mount_path: str) -> NamedTuple('outputs', org_dataset_path=str, dataset_path=str):\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    org_dataset_path = mount_path + '/org_dataset'\n",
    "    dataset['train'].to_json(org_dataset_path + '/camel_math_train.json')\n",
    "    dataset['test'].to_json(org_dataset_path + '/camel_math_test.json')\n",
    "    dataset['validation'].to_json(org_dataset_path + '/camel_math_val.json')\n",
    "    \n",
    "    def convert_to_message_function(example):\n",
    "        prompt = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': example['message_1']\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': example['message_2']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        return prompt\n",
    "        \n",
    "    organized_dataset = dataset.map(convert_to_message_function, remove_columns=dataset.column_names['train'])\n",
    "    print(f\"***Formatted Sample data***\\n {organized_dataset['train'][0]}\")\n",
    "\n",
    "    dataset_path = mount_path + '/camel_math_organized'\n",
    "    organized_dataset.save_to_disk(dataset_dict_path=dataset_path)\n",
    "\n",
    "    return_value = NamedTuple('outputs', org_dataset_path=str, dataset_path=str)\n",
    "    return return_value(org_dataset_path, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92779185-9b8f-482b-b31f-9d046dcb2ffe",
   "metadata": {},
   "source": [
    "### Component #2 Run SFT with prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf021ee-2096-43da-9466-14a1ac918492",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='geuntakroh/transformers-pytorch-gpu:v4.57.1',\n",
    ")\n",
    "def finetuning_llm(dataset_path: str) -> NamedTuple('outputs', last_run_id=str, model_name=str):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from datasets import load_from_disk\n",
    "    from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "    import torch\n",
    "    from transformers.integrations import MLflowCallback\n",
    "    import re\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    # import logging\n",
    "    from transformers.utils import logging\n",
    "    logger = logging.get_logger(__name__)\n",
    "    \n",
    "    \n",
    "    ## Set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ## Initialize Model\n",
    "    model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "    logger.info(f\"Chat Template: {tokenizer.chat_template}\")\n",
    "    \n",
    "    ## Load the Dataset\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    ## Set Callback for MLFLOW token renewing\n",
    "    run_name = model.name_or_path.split('/')[1] + '-' + time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "    logger.info(f\"MLFLOW Run Name: {run_name}\")\n",
    "        \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = \"http://mlflow.mlflow.svc.cluster.local:5000\" #mlflow_url\n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://local-s3-service.ezdata-system.svc.cluster.local:30000\" # mlflow_s3_url\n",
    "    os.environ['MLFLOW_EXPERIMENT_NAME'] = \"finetuning-llm-kfp\" #mlflow_experiment\n",
    "    os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n",
    "    os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'\n",
    "    os.environ['HF_MLFLOW_LOG_ARTIFACTS'] = 'True'\n",
    "\n",
    "    \n",
    "    def renew_token(step: str = None):\n",
    "        with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "            AUTH_TOKEN = file.read()\n",
    "            os.environ['MLFLOW_TRACKING_TOKEN']=AUTH_TOKEN\n",
    "            os.environ[\"AWS_ACCESS_KEY_ID\"] = AUTH_TOKEN\n",
    "            os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "            if step is not None:\n",
    "                logger.info(f\"AUTH_TOKEN - {step} : [{AUTH_TOKEN[-20:]}]\")\n",
    "            else:\n",
    "                logger.info(f\"AUTH_TOKEN : [{AUTH_TOKEN[-20:]}]\")\n",
    "\n",
    "    renew_token()\n",
    "    \n",
    "    class CustomizedMLflowCallback(MLflowCallback):\n",
    "        def on_log(self, args, state, control, logs, model=None, **kwargs):\n",
    "            renew_token('on_log')\n",
    "            super().on_log(args, state, control, logs, model=None, **kwargs)\n",
    "    \n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            renew_token('on_save')\n",
    "            import boto3 \n",
    "            if boto3.DEFAULT_SESSION is not None:\n",
    "                logger.info(f\"boto3 : {boto3.DEFAULT_SESSION.get_credentials().access_key[-20:]}, Env : {os.environ['AWS_ACCESS_KEY_ID'][-20:]}\")\n",
    "                if boto3.DEFAULT_SESSION.get_credentials().access_key != os.environ['AWS_ACCESS_KEY_ID']:\n",
    "                    boto3.DEFAULT_SESSION = None\n",
    "                    logger.info(\"Initialize Default Session of Boto3 to update Credential from Environment Variable!\")\n",
    "                \n",
    "            super().on_save(args, state, control, **kwargs)\n",
    "\n",
    "    ## Define PEFT ( LoRA ) Settings\n",
    "    from peft import LoraConfig\n",
    "    from trl import SFTConfig, SFTTrainer\n",
    "    \n",
    "    ## Configure LoRA parameters\n",
    "    rank_dimension = 4 # r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "    lora_alpha = 8 # lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "    lora_dropout = 0.05 # lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "        lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "        lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "        bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "        target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "        task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    "    )\n",
    "\n",
    "    ## Define trainer Class\n",
    "    max_steps = 100 #2000\n",
    "    # num_train_epochs=1\n",
    "    logging_steps=10\n",
    "    save_steps= 100 #1000\n",
    "    eval_steps=1000\n",
    "    per_device_train_batch_size=4\n",
    "    max_seq_length = 1024\n",
    "    save_total_limit=2\n",
    "    model_dir = model_name.split('/')[1]\n",
    "\n",
    "    # Configure trainer\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=model_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        max_steps=max_steps,\n",
    "        # num_train_epochs=num_train_epochs,\n",
    "        save_total_limit=save_total_limit,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        learning_rate=5e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        report_to=[],\n",
    "        max_length=max_seq_length,  # Maximum sequence length\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        processing_class=tokenizer,\n",
    "        peft_config=peft_config,  # LoRA configuration\n",
    "    )\n",
    "    trainer.add_callback(CustomizedMLflowCallback)\n",
    "\n",
    "    ## Check Dataset whether chat_template is  applied\n",
    "    for batch in trainer.get_train_dataloader():\n",
    "        logger.info(batch.keys())\n",
    "        logger.info(f\"***Chat Template Applied Dataset***\\n {tokenizer.decode(batch['input_ids'][0],skip_special_tokens=False)}\")\n",
    "        break\n",
    "\n",
    "    ## Run Training \n",
    "    trainer.train()\n",
    "    trainer.save_model(model_dir + '-savedir')\n",
    "    \n",
    "    ## Log artifacts to MLFLOW\n",
    "    import mlflow\n",
    "\n",
    "    ## Get the ID of the MLflow Run that was automatically created above\n",
    "    last_run_id = mlflow.last_active_run().info.run_id\n",
    "    renew_token('Final Model Logging')\n",
    "        \n",
    "    with mlflow.start_run(run_id=last_run_id):\n",
    "        mlflow.log_params(peft_config.to_dict())\n",
    "        # mlflow.log_artifacts(model_dir + '-savedir', artifact_path=model_dir + '-savedir')\n",
    "        \n",
    "        mlflow.transformers.log_model(\n",
    "          transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "          artifact_path=model_dir + '-savedir',  # This is a relative path to save model files within MLflow run\n",
    "        )\n",
    "    \n",
    "    return_value = NamedTuple('outputs', last_run_id=str, model_name=str)\n",
    "    return return_value(last_run_id, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe74067-7645-4e91-9244-eb88a6599fe8",
   "metadata": {},
   "source": [
    "### Component #3 Serving LoRa in MLIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd416098-8fc3-44fe-921b-2e960b1eaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='geuntakroh/transformers-pytorch-gpu:v4.57.1',\n",
    ")\n",
    "def deploy_lora_adapter(last_run_id: str,mlis_url: str,namespace: str,full_model_name:str,bucket_name: str) -> str:\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import logging\n",
    "    import os\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    model_name = full_model_name.split('/')[1]\n",
    "    model_name_lower = model_name.lower()\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = \"http://mlflow.mlflow.svc.cluster.local:5000\" #mlflow_url\n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://local-s3-service.ezdata-system.svc.cluster.local:30000\" # mlflow_s3_url\n",
    "    os.environ['MLFLOW_TRACKING_INSECURE_TLS'] = 'true'\n",
    "    os.environ['MLFLOW_S3_IGNORE_TLS'] = 'true'\n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "        os.environ['MLFLOW_TRACKING_TOKEN']=AUTH_TOKEN\n",
    "    \n",
    "    # Get the base artifact URI (S3 path)\n",
    "    client = MlflowClient()\n",
    "    run = client.get_run(last_run_id)\n",
    "    artifact_uri = run.info.artifact_uri\n",
    "    logger.info(f\"Artifact URI: {artifact_uri}\")\n",
    "    \n",
    "    # For a specific artifact file/folder\n",
    "    artifact_path = model_name + \"-savedir/peft\"  # or any artifact path\n",
    "    \n",
    "    full_artifact_uri = f\"{artifact_uri}/{artifact_path}\"\n",
    "    logger.info(f\"Full artifact URI: {full_artifact_uri}\")\n",
    "\n",
    "    # Intialize API client for MLIS's rest API\n",
    "    import aiolirest\n",
    "    from aioli.common import api, util\n",
    "    from aioli.common.api import authentication\n",
    "    \n",
    "    host_url = mlis_url\n",
    "    host = util.prepend_protocol(host_url)\n",
    "    configuration = authentication.get_rest_config(host)\n",
    "    # token = util.get_aioli_user_token_from_env()\n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as f:\n",
    "        token = f.read()\n",
    "    logger.info(f\"HOST: {host} / Token: {token}\")\n",
    "\n",
    "    configuration.api_key[\"ApiKeyAuth\"] = \"Bearer \" + token\n",
    "    restclient = aiolirest.ApiClient(configuration)\n",
    "    \n",
    "    # Create registry \n",
    "    from aiolirest.models.trained_model_registry_request import TrainedModelRegistryRequest\n",
    "    \n",
    "    api_instance = aiolirest.RegistriesApi(restclient)\n",
    "    \n",
    "    registry_request = TrainedModelRegistryRequest(\n",
    "        name=model_name + '-internal-s3',\n",
    "        bucket=bucket_name,\n",
    "        endpointUrl='http://local-s3-service.ezdata-system.svc.cluster.local:30000',\n",
    "        type='s3',\n",
    "        accessKey=token,\n",
    "        secretKey='s3',\n",
    "        insecureHttps=False,\n",
    "        project=None,\n",
    "    )\n",
    "    api_instance.registries_post(registry_request)\n",
    "    logger.info(registry_request.name)\n",
    "\n",
    "    # Create packaged model\n",
    "    from aiolirest.models.configuration_resources import ConfigurationResources\n",
    "    from aiolirest.models.deployment_model_version import DeploymentModelVersion\n",
    "    from aiolirest.models.packaged_model import PackagedModel\n",
    "    from aiolirest.models.packaged_model_request import PackagedModelRequest\n",
    "    from aiolirest.models.resource_profile import ResourceProfile\n",
    "    from argparse import Namespace\n",
    "\n",
    "    api_instance = aiolirest.PackagedModelsApi(restclient)\n",
    "\n",
    "    config = {\n",
    "        'requests_cpu': '1',\n",
    "        'requests_gpu': '1',\n",
    "        'requests_memory': '4Gi',\n",
    "        'limits_cpu': '4',\n",
    "        'limits_gpu': '1',\n",
    "        'limits_memory': '8Gi',\n",
    "        'enable_caching': False,\n",
    "        'disable_caching': False,\n",
    "        'metadata': {\n",
    "            'modelCategory=llm'\n",
    "        },\n",
    "        'env': {\n",
    "            'a=b',\n",
    "            'c=d',\n",
    "        },\n",
    "        'arg': [\n",
    "            '--model',\n",
    "            full_model_name,\n",
    "            '--port',\n",
    "            '8080',\n",
    "            '--dtype=half',\n",
    "            '--gpu-memory-utilization',\n",
    "            '0.8',\n",
    "            '--enable-lora',\n",
    "            '--lora-modules',\n",
    "            '{\"name\":\"math-lora\",\"path\":\"/mnt/models\",\"base_model_name\":\"' + full_model_name + '\"}',\n",
    "        ]\n",
    "    }\n",
    "    args = Namespace(**config)\n",
    "    requests = ResourceProfile(\n",
    "        cpu=args.requests_cpu, gpu=args.requests_gpu, memory=args.requests_memory\n",
    "    )\n",
    "    limits = ResourceProfile(\n",
    "        cpu=args.limits_cpu, gpu=args.limits_gpu, memory=args.limits_memory\n",
    "    )\n",
    "    resources = ConfigurationResources(gpuType=None, requests=requests, limits=limits)\n",
    "    logger.info(f\"Resources: {resources}\")\n",
    "    from aioli.common.util import (\n",
    "        construct_arguments,\n",
    "        construct_environment,\n",
    "        construct_metadata,\n",
    "        launch_dashboard,\n",
    "    )\n",
    "    logger.info(construct_metadata(args, {}))\n",
    "    logger.info(construct_environment(args))\n",
    "    logger.info(construct_arguments(args))\n",
    "\n",
    "    packaged_model_request = PackagedModelRequest(\n",
    "        name=model_name,\n",
    "        description=model_name,\n",
    "        url=full_artifact_uri,\n",
    "        image='vllm/vllm-openai:v0.8.5',\n",
    "        resources=resources,\n",
    "        modelFormat='custom',\n",
    "        metadata=construct_metadata(args, {}),\n",
    "        arguments=construct_arguments(args),\n",
    "        registry=registry_request.name,\n",
    "    )\n",
    "    \n",
    "    if args.enable_caching:\n",
    "        r.caching_enabled = True\n",
    "    \n",
    "    if args.disable_caching:\n",
    "        r.caching_enabled = False\n",
    "    \n",
    "    response = api_instance.models_post(packaged_model_request)\n",
    "    logger.info(response)\n",
    "\n",
    "    # Deploy Model\n",
    "    from aioli.common.util import (\n",
    "        construct_arguments,\n",
    "        construct_environment,\n",
    "        launch_dashboard,\n",
    "    )\n",
    "    from aiolirest.models.autoscaling import Autoscaling\n",
    "    from aiolirest.models.deployment import Deployment, DeploymentState\n",
    "    from aiolirest.models.deployment_request import DeploymentRequest\n",
    "    from aiolirest.models.event_info import EventInfo\n",
    "    from aiolirest.models.security import Security\n",
    "    api_instance = aiolirest.DeploymentsApi(restclient)\n",
    "\n",
    "    config = {\n",
    "        'autoscaling_target': 1,\n",
    "        'autoscaling_metric': 'rps',\n",
    "        'autoscaling_max_replicas': 1,\n",
    "        'autoscaling_min_replicas': 1,\n",
    "    }\n",
    "    args = Namespace(**config)\n",
    "    auto = Autoscaling(\n",
    "        metric=args.autoscaling_metric,\n",
    "    )\n",
    "    \n",
    "    if args.autoscaling_target is not None:\n",
    "        auto.target = args.autoscaling_target\n",
    "    \n",
    "    if args.autoscaling_max_replicas is not None:\n",
    "        auto.max_replicas = args.autoscaling_max_replicas\n",
    "    \n",
    "    if args.autoscaling_min_replicas is not None:\n",
    "        auto.min_replicas = args.autoscaling_min_replicas\n",
    "\n",
    "    sec = Security(authenticationRequired=True)\n",
    "    logger.info(f\"{auto} / {sec}\")\n",
    "\n",
    "    deployment_request = DeploymentRequest(\n",
    "        name=model_name_lower,\n",
    "        model=model_name,\n",
    "        security=sec,\n",
    "        namespace=namespace,\n",
    "        autoScaling=auto,\n",
    "    )\n",
    "    results = api_instance.deployments_post(deployment_request)\n",
    "    logger.info(results)\n",
    "\n",
    "    from aioli.cli import deployment\n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        model_status = deployment.lookup_deployment(model_name_lower,api_instance).status\n",
    "        logger.info(model_status)\n",
    "        time.sleep(10)\n",
    "        if model_status == 'Ready':\n",
    "            logger.info(\"Model is Ready!\")\n",
    "            break\n",
    "        elif model_status != 'Deploying' and model_status != 'Unknown':\n",
    "            logger.info('Something went wrong, Check the deployment in the MLIS!')\n",
    "            break\n",
    "            \n",
    "    model_endpoint_url = deployment.lookup_deployment(model_name_lower,api_instance).state.endpoint\n",
    "\n",
    "    return model_endpoint_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdc51c-eea3-4af7-b461-f9ca92ff3b47",
   "metadata": {},
   "source": [
    "### Component #4 Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95bf06c2-12af-4fcf-b3e7-7e9f88a7fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='geuntakroh/transformers-pytorch-gpu:v4.57.1',\n",
    ")\n",
    "def sample_inferencing(endpoint_url: str):\n",
    "    import requests\n",
    "    import logging\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {AUTH_TOKEN}\"\n",
    "    }\n",
    "\n",
    "    route = '/v1/models'\n",
    "    models_response = requests.get(endpoint_url+route,headers=headers,verify=False)\n",
    "    sample_prompt = \"In a 90-minute soccer game, Mark played 20 minutes, then rested after. He then played for another 35 minutes. How long was he on the sideline?\"\n",
    "    \n",
    "    for model in models_response.json()['data']:\n",
    "        logger.info(model['id'])\n",
    "        payload = {\n",
    "            \"model\": model['id'],\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"you are a helpful math tutor, solve the question step by step\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": sample_prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        route = '/v1/chat/completions'\n",
    "        chat_response = requests.post(endpoint_url+route,headers=headers,verify=False,json=payload)\n",
    "        logger.info(f\"*** {model['id']} ***\\n{chat_response.json()['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b71c51-78d1-487e-974c-2a6d965a7d6f",
   "metadata": {},
   "source": [
    "### Component #5 Benchmark the LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9db46f07-6338-4c8a-bb40-b06606f07186",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='geuntakroh/lighteval:v0.12.1',\n",
    "    install_kfp_package=False,\n",
    ")\n",
    "def lighteval_benchmark(mount_path: str,endpoint_url: str,org_dataset_path: str):\n",
    "    script = f\"\"\"\n",
    "from lighteval.tasks.requests import Doc\n",
    "from lighteval.metrics.metrics import Metrics\n",
    "from lighteval.tasks.lighteval_task import LightevalTaskConfig\n",
    "\n",
    "\n",
    "def prompt_fn(line: dict, task_name: str):\n",
    "    query = line[\"message_1\"]\n",
    "    choices = [line[\"message_2\"]]\n",
    "    return Doc(\n",
    "        task_name=task_name,\n",
    "        query=query,\n",
    "        choices=choices,\n",
    "        gold_index=0,\n",
    "    )\n",
    "\n",
    "custom_task = LightevalTaskConfig(\n",
    "    name=\"custom_task\",\n",
    "    prompt_function=prompt_fn,\n",
    "    hf_repo=\"{org_dataset_path}\",\n",
    "    hf_subset=\"\",\n",
    "    evaluation_splits=[\"validation\"],\n",
    "    few_shots_split='train',\n",
    "    few_shots_select='random_sampling_from_train',\n",
    "    generation_size=1024,\n",
    "    metrics=[Metrics.bleu],\n",
    "    stop_sequence=[],\n",
    "    version=0,\n",
    ")\n",
    "\n",
    "TASKS_TABLE = [custom_task]\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(mount_path + '/custom_task.py','w') as f:\n",
    "        f.write(script)\n",
    "\n",
    "    # Parse the URL\n",
    "    from urllib.parse import urlparse\n",
    "    parsed = urlparse(endpoint_url)\n",
    "    hostname = parsed.hostname.split('.')\n",
    "    \n",
    "    # Split by dots and get the first two parts\n",
    "    parts = hostname[0].split('-predictor-')\n",
    "    model_name = parts[0] + \"-predictor-00001\"  # smollm2-360m-instruct-predictor\n",
    "    project_name = parts[1]  # geun-tak-roh-hp-b3801707\n",
    "    print(f\"http://{model_name}.{project_name}.svc.cluster.local\")\n",
    "    \n",
    "    \n",
    "    import lighteval\n",
    "    from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "    from lighteval.models.endpoints.litellm_model import LiteLLMModelConfig\n",
    "    from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters\n",
    "    from lighteval.models.model_input import GenerationParameters\n",
    "    \n",
    "    with open('/etc/secrets/ezua/.auth_token','r') as file:\n",
    "        token = file.read()\n",
    "    \n",
    "    evaluation_tracker = EvaluationTracker(\n",
    "        output_dir=\"/tmp/results-custom\",\n",
    "        save_details=True,\n",
    "    )\n",
    "    \n",
    "    pipeline_params = PipelineParameters(\n",
    "        launcher_type=ParallelismManager.NONE,\n",
    "        custom_tasks_directory=mount_path + '/custom_task.py',  # Set to path if using custom tasks\n",
    "        max_samples=500\n",
    "    )\n",
    "    \n",
    "    model_config = LiteLLMModelConfig(\n",
    "        model_name=\"hosted_vllm/math-lora\",\n",
    "        provider='vllm',\n",
    "        base_url= f\"http://{model_name}.{project_name}.svc.cluster.local\" + \"/v1\",\n",
    "        api_key=token,\n",
    "        generation_parameters=GenerationParameters(\n",
    "            temperature=0.5,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    task = \"custom_task|0\"\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        tasks=task,\n",
    "        pipeline_parameters=pipeline_params,\n",
    "        evaluation_tracker=evaluation_tracker,\n",
    "        model_config=model_config,\n",
    "    )\n",
    "    \n",
    "    pipeline.evaluate()\n",
    "    pipeline.show_results()\n",
    "    # pipeline.save_and_push_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4fbe6e-4080-46f0-a3c2-1ab87b937ef2",
   "metadata": {},
   "source": [
    "# 2. Define the Pipeline\n",
    "A pipeline is a description of a machine learning (ML) workflow, including all of the components in the workflow and how the components relate to each other in the form of a graph. The pipeline configuration includes the definition of the inputs (parameters) required to run the pipeline and the inputs and outputs of each component.\n",
    "\n",
    "When you run a pipeline, the system launches one or more Kubernetes Pods corresponding to the steps (components) in your workflow (pipeline). The Pods start Docker containers, and the containers in turn start your programs.\n",
    "\n",
    "After developing your pipeline, you can upload your pipeline using the Kubeflow Pipelines UI or the Kubeflow Pipelines SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e163354-190d-43f9-ac23-f7804e5f521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "669fefa2-9b60-49e8-a649-46c17389fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"SFT_PEFT_pipeline\"\n",
    ")\n",
    "def llm_SFT_PEFT_pipeline(\n",
    "    dataset_name: str,\n",
    "    mount_path: str,\n",
    "    dataset_pvc_name: str,\n",
    "    dataset_pvc_size: str,\n",
    "    current_sc: str,\n",
    "    namespace: str,\n",
    "    mlis_url: str,\n",
    "    bucket_name: str\n",
    ") -> str:\n",
    "    pvc1 = kubernetes.CreatePVC(\n",
    "        pvc_name=dataset_pvc_name,\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size=dataset_pvc_size,\n",
    "        storage_class_name=current_sc, # gl4fs-system for PCAI\n",
    "    )\n",
    "    # write to the PVC\n",
    "    target_path = '/data'\n",
    "    task1 = preprocess_dataset(dataset_name=dataset_name,mount_path=mount_path)\n",
    "    task1.set_cpu_limit(\"8\")\n",
    "    task1.set_memory_limit(\"16Gi\")\n",
    "    kubernetes.mount_pvc(\n",
    "        task1,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path=target_path,\n",
    "    )\n",
    "\n",
    "    task2 = finetuning_llm(dataset_path=task1.outputs['dataset_path'])\n",
    "    task2.set_accelerator_type(\"nvidia.com/gpu\").set_accelerator_limit(\"1\")\n",
    "    task2.set_cpu_limit(\"8\")\n",
    "    task2.set_memory_limit(\"16Gi\")\n",
    "    kubernetes.mount_pvc(\n",
    "        task2,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path=target_path,\n",
    "    )\n",
    "    ### Please add, if you are using AIE in versions prior to 1.9\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task2,\n",
    "        annotation_key=\"hpe-ezua/add-auth-token\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task2,\n",
    "        annotation_key=\"hpe-ezua/disable-sc\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "\n",
    "    task3 = deploy_lora_adapter(last_run_id=task2.outputs['last_run_id'],mlis_url=mlis_url,namespace=namespace,full_model_name=task2.outputs['model_name'],bucket_name=bucket_name)\n",
    "    ### Please add, if you are using AIE in versions prior to 1.9\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task3,\n",
    "        annotation_key=\"hpe-ezua/add-auth-token\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task3,\n",
    "        annotation_key=\"hpe-ezua/disable-sc\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "    task4 = sample_inferencing(endpoint_url=task3.output)\n",
    "    ### Please add, if you are using AIE in versions prior to 1.9\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task4,\n",
    "        annotation_key=\"hpe-ezua/add-auth-token\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "    kubernetes.add_pod_annotation(\n",
    "        task=task4,\n",
    "        annotation_key=\"hpe-ezua/disable-sc\",\n",
    "        annotation_value=\"true\"\n",
    "    )\n",
    "    task5 = lighteval_benchmark(mount_path=mount_path,endpoint_url=task3.output,org_dataset_path=task1.outputs['org_dataset_path'])\n",
    "    task5.set_cpu_limit(\"8\")\n",
    "    task5.set_memory_limit(\"16Gi\")\n",
    "    kubernetes.mount_pvc(\n",
    "        task5,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path=target_path,\n",
    "    )\n",
    "\n",
    "    task5.set_env_variable('GIT_PYTHON_REFRESH','quiet')\n",
    "\n",
    "    return task3.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753486e0-8995-421e-90c1-a53705c91b4d",
   "metadata": {},
   "source": [
    "# 3. Launch the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8472198-6e50-4475-a0e5-946975f5e182",
   "metadata": {},
   "source": [
    "### Collect necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7e7b91a-9c2e-4771-926f-45bb3c7ecdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found endpoint for s3 via: environment_global.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\", verify=False)\n",
    "buckets = s3.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    if 'mlflow' in bucket['Name']:\n",
    "      bucket_name = bucket['Name']\n",
    "\n",
    "dataset_name = \"rhgt1996/camel_math_split\"\n",
    "mount_path = \"/data\"\n",
    "dataset_pvc_name = \"camel-math-dataset\"\n",
    "dataset_pvc_size = '3Gi'\n",
    "current_sc = os.popen(\"kubectl get pvc user-pvc -o=jsonpath='{.spec.storageClassName}'\").read()\n",
    "namespace_cur = os.popen(\"kubectl get pvc user-pvc -o=jsonpath='{.metadata.namespace}'\").read()\n",
    "mlis_url = \"http://aioli-master-service-hpe-mlis.mlis.svc.cluster.local:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecc37e07-213f-4ef4-b1f7-bbecb06667c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp_client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dc210-f8d0-4088-862d-cbf455d0d867",
   "metadata": {},
   "source": [
    "### Execute Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6f9369a-7362-4ee5-abf2-f66032874a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/624af3ae-6185-4b38-9f38-ee5cde042845\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/610a43e8-8fb6-4c0b-b2a2-ba09b30b4c22\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=610a43e8-8fb6-4c0b-b2a2-ba09b30b4c22)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp_client.create_run_from_pipeline_func(\n",
    "    llm_SFT_PEFT_pipeline,\n",
    "    arguments={\n",
    "        'dataset_name': dataset_name,\n",
    "        'mount_path': mount_path,\n",
    "        'dataset_pvc_name': dataset_pvc_name,\n",
    "        'dataset_pvc_size': dataset_pvc_size,\n",
    "        'current_sc': current_sc,\n",
    "        'namespace': namespace_cur,\n",
    "        'mlis_url': mlis_url,\n",
    "        'bucket_name': bucket_name\n",
    "    },\n",
    "    experiment_name=\"e2e-SFT-pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ebe536e-d327-4dfc-b313-60f49f45ee9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      STATUS    AGE    MESSAGE\n",
      "sft-peft-pipeline-57xms   Failed    3m1s   \n",
      "sft-peft-pipeline-twdk9   Running   2s     \n"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "060fe551-9d31-47d4-a55a-c4373a0f0129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      STATUS      AGE   MESSAGE\n",
      "sft-peft-pipeline-twdk9   Succeeded   15m   \n"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a291-cae5-4c0f-8dc9-8db2027ec2c8",
   "metadata": {},
   "source": [
    "<img src=\"../assets/kfp_pipeline.png\" alt=\"metrics in mlflow\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
